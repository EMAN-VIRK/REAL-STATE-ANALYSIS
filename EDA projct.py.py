# -*- coding: utf-8 -*-
"""Copy of EDA PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1phkUXuRZXs1TCpvYzK7WddQDoyim7Adh

**# Import Dataset**
"""

import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/Scarped Zameen.csv")
df

"""# **Basic Data Analysis**"""

display (df.head(5)) # Show the first 5 rows of the dataset
display(df.info()) # Display basic information about the dataset
display(df.describe()) #statistical summary of data
# Unique values in categorical columns
print("\nUnique values in categorical columns:")
for col in ['Type', 'Purpose','City']:  # Example categorical columns
    print(f"\nColumn '{col}':")
    print(df[col].unique())
    print(df[col].value_counts())

"""# **Standardization of Data**
we implemented various functions to standardise the collected information. These included function such as to convert prices into a normalised form in PKR, accounting for variations like Lakhs and Millions and standardise of property sizes to square feet, overcoming inconsistent units such as Marla and Kanal.
"""

import re
import numpy as np

def clean_price(price):
    if isinstance(price, str):
        price = price.replace("PKR", "").replace(",", "").replace("\xa0", "").strip().lower()
        try:
            if "crore" in price:
                return float(re.sub(r"[^0-9.]", "", price)) * 10000000
            elif "lakh" in price:
                return float(re.sub(r"[^0-9.]", "", price)) * 100000
            elif "million" in price:
                return float(re.sub(r"[^0-9.]", "", price)) * 1000000
            elif "thousand" in price:
                return float(re.sub(r"[^0-9.]", "", price)) * 1000
            elif "arab" in price:
                return float(re.sub(r"[^0-9.]", "", price)) * 1000000000
            else:
                return float(re.sub(r"[^0-9.]", "", price))
        except:
            return np.nan
    return np.nan

df['Price'] = df['Price'].apply(clean_price)
print(df['Price'].tail())
print(df['Price'].dtype)

"""# **Standardize the Area Column**"""

import re
import numpy as np

def convert_area_to_sqft(area):
    if not isinstance(area, str):
        return np.nan

    area = area.replace('\n', ' ') \
               .replace('\xa0', ' ') \
               .replace(',', '') \
               .replace('¬∑', '') \
               .lower().strip()

    total_sqft = 0

    # Match Kanal
    kanal = re.findall(r'(\d+\.?\d*)\s*kanal', area)
    if kanal:
        total_sqft += sum([float(k) * 5445 for k in kanal])

    # Match Marla
    marla = re.findall(r'(\d+\.?\d*)\s*marla', area)
    if marla:
        total_sqft += sum([float(m) * 272.25 for m in marla])

    # Match Sq. Yd.
    sq_yd = re.findall(r'(\d+\.?\d*)\s*sq\.?\s*yd', area)
    if sq_yd:
        total_sqft += sum([float(y) * 9 for y in sq_yd])

    # Match Sq. Ft.
    sq_ft = re.findall(r'(\d+\.?\d*)\s*sq\.?\s*ft', area)
    if sq_ft:
        total_sqft += sum([float(f) for f in sq_ft])

    # Match Acre
    acre = re.findall(r'(\d+\.?\d*)\s*acre', area)
    if acre:
        total_sqft += sum([float(a) * 43560 for a in acre])

    return total_sqft if total_sqft > 0 else np.nan

df['Area'] = df['Area'].apply(convert_area_to_sqft)

print(df['Area'].sample(10))
print(df['Area'].dtype)

"""# **Drop Columns with All Missing Values**
These are useless columns with 100% NaN, and we can safely remove them.
"""

df.dropna(axis=1, how='all', inplace=True)
print(df.shape)        # See new shape
print(df.columns)      # List of remaining useful columns
display(df.info()) # Display basic information about the dataset

df.dtypes

"""# **Fix Data Types (Final Cleanup Before Imputation)**"""

df['Bedrooms'] = pd.to_numeric(df['Bedrooms'], errors='coerce')
df['Bathrooms'] = pd.to_numeric(df['Bathrooms'], errors='coerce')

df.dtypes

"""# **Handle Missing Values**"""

df.isnull().sum()
#age % missing value-- no of missing value/ total data *100

"""# **Percentage of Missing Values**"""

#% of missing values in a column
missing_values = df.isnull().sum() # how many missing values in the column
missing_percentage = (missing_values / len(df)) * 100 # len----> no of rows

print("\nMissing Value Percentage:\n", missing_percentage)

"""## **üìä Bedrooms Column Analysis: Distribution, Outliers, and Imputation Strategy**

"""

# Import necessary libraries for visualization and statistics
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew
import pandas as pd

# Ensure 'Bedrooms' column is numeric; convert any non-numeric values to NaN
df['Bedrooms'] = pd.to_numeric(df['Bedrooms'], errors='coerce')

# Plotting the histogram to visualize the distribution of bedrooms
plt.figure(figsize=(10, 5))  # Set figure size
sns.histplot(df["Bedrooms"].dropna(), bins=15, kde=True, color='skyblue')  # Histogram with KDE curve
plt.title("Distribution of Bedrooms")
plt.xlabel("Number of Bedrooms")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

# Boxplot to detect outliers in the 'Bedrooms' column
plt.figure(figsize=(8, 2))  # Horizontal boxplot
sns.boxplot(x=df["Bedrooms"].dropna(), color='orange')
plt.title("Boxplot of Bedrooms")
plt.show()

# Calculate skewness to determine the distribution shape
bedroom_skew = skew(df["Bedrooms"].dropna())
print(f"Skewness of Bedrooms: {bedroom_skew:.2f}")

# Suggest imputation strategy based on skewness
if bedroom_skew > 1:
    print("Highly Right-Skewed ‚Üí Use Median for imputation")
elif bedroom_skew < -1:
    print("Highly Left-Skewed ‚Üí Use Median for imputation")
else:
    print("Approximately Normal ‚Üí Use Mean or Median")

# Impute missing values in the 'Bedrooms' column using the median (robust to outliers and slight skewness)
df['Bedrooms'] = df['Bedrooms'].fillna(df['Bedrooms'].median())

print(df['Bedrooms'].isnull().sum())

"""**# Outlier Detection Using IQR Method for 'Bedrooms' Column**"""

def detect_outliers_iqr(data, column_name):
    """
    Detects outliers in a specified column using the Interquartile Range (IQR) method.
    Prints the outliers and their count, and returns a DataFrame of outlier rows.

    Parameters:
    - data: DataFrame
    - column_name: str, the name of the numeric column to analyze
    """
    # Calculate Q1 (25th percentile) and Q3 (75th percentile)
    Q1 = data[column_name].quantile(0.25)
    Q3 = data[column_name].quantile(0.75)

    # Calculate the Interquartile Range (IQR)
    IQR = Q3 - Q1

    # Define lower and upper bounds for outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filter rows where values are out of bounds
    outliers = data[(data[column_name] < lower_bound) | (data[column_name] > upper_bound)]

    # Print details
    print(f"\nOutliers in '{column_name}':")
    print(outliers[[column_name]])

    print(f"\nCount of outliers in '{column_name}': {outliers.shape[0]}")

    return outliers

#  Ensure 'Bedrooms' column is numeric to avoid errors
df['Bedrooms'] = pd.to_numeric(df['Bedrooms'], errors='coerce')

# üîç Detect outliers in the 'Bedrooms' column using the IQR method
bedroom_outliers = detect_outliers_iqr(df, 'Bedrooms')

#df[df["Bedrooms"]>=10]
df[df['Bedrooms'] >= 10][['Price', 'Area', 'Bedrooms', 'City', 'Purpose','Location','Title',]]

import pandas as pd

# Sample function to validate high-priced rentals
def fix_and_validate_price(row):
    price = row['Price']
    area = row['Area']
    purpose = row['Purpose']

    if purpose == 'For Rent' and price > 2_000_000:
        if price > 5_000_000:
            return "‚ùó Possible Price Typo: Should be 8.1 Lakh?"
        else:
            return "‚úÖ High-end Rent ‚Äì OK"
    return "‚úÖ Valid"

# Apply the function to your dataframe
df['Price_Validation'] = df.apply(fix_and_validate_price, axis=1)

# View results
df[['Price', 'Area', 'Price_Numeric','Bedrooms', 'Area_SqYd', 'Purpose', 'Price_Validation','Type']]

df['Type'].value_counts()

print(df.columns.tolist())

def convert_price(price):
    price = str(price).replace("PKR", "").replace(",", "").strip()
    if "Crore" in price:
        return float(price.replace("Crore", "").strip()) * 1e7
    elif "Lakh" in price:
        return float(price.replace("Lakh", "").strip()) * 1e5
    elif "Thousand" in price:
        return float(price.replace("Thousand", "").strip()) * 1e3
    else:
        try:
            return float(price)
        except:
            return None

df['Price_Numeric'] = df['Price'].apply(convert_price)

df[['Price', 'Price_Numeric']].head()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
sns.histplot(df[df['Type'] == 'Room']['Price_Numeric'], bins=30, kde=True)
plt.title('Price Distribution of "Room" Listings')
plt.xlabel('Price (PKR)')
plt.ylabel('Count')
plt.xlim(0, 5000000)  # Adjust if needed
plt.show()

plt.figure(figsize=(10, 5))
sns.histplot(df[(df['Type'] == 'Room') & (df['Price_Numeric'] <= 200000)]['Price_Numeric'], bins=30, kde=True)
plt.title('Filtered Price Distribution of "Room" Listings (Under 2 Lakh PKR)')
plt.xlabel('Price (PKR)')
plt.ylabel('Count')
plt.show()

df[df['Type'] == 'Room']\
  .sort_values('Price_Numeric', ascending=False)\
  .head(5)[['Description', 'Price', 'Area', 'Purpose']]

"""LOWER pORTION/UPPER PORTION"""

df[df['Type'] == 'Lower Portion'][['Price', 'Purpose', 'Location', 'Description',]].head(100)

"""# **Drop the Floors Column**"""

#  Drop 'Floors' column due to high missing value percentage (>50%)
# Drop 'Floors' only if it's present
df.drop(columns=[col for col in ['Floors'] if col in df.columns], inplace=True)

#  Check if 'Floors' is successfully dropped
print('Floors' in df.columns)  # Should return False

"""# **Cleaning: Bathrooms Column**"""

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew
import pandas as pd

#  Convert Bathrooms to numeric (if it's object)
df['Bathrooms'] = pd.to_numeric(df['Bathrooms'], errors='coerce')

#  Plot Histogram & Boxplot to check distribution and outliers
plt.figure(figsize=(10, 5))
sns.histplot(df["Bathrooms"].dropna(), bins=15, kde=True, color='lightblue')
plt.title("Distribution of Bathrooms")
plt.xlabel("Number of Bathrooms")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

plt.figure(figsize=(8, 2))
sns.boxplot(x=df["Bathrooms"].dropna(), color='orange')
plt.title("Boxplot of Bathrooms")
plt.show()

#  Check Skewness to decide imputation method
bathroom_skew = skew(df["Bathrooms"].dropna())
print(f"Skewness of Bathrooms: {bathroom_skew:.2f}")

if bathroom_skew > 1:
    print("Highly Right-Skewed ‚Üí Use Median for imputation")
elif bathroom_skew < -1:
    print("Highly Left-Skewed ‚Üí Use Median for imputation")
else:
    print("Approximately Normal ‚Üí Use Mean or Median")

#  Impute missing values with median (if skewed)
df['Bathrooms'] = df['Bathrooms'].fillna(df['Bathrooms'].median())

#  Optional: Detect outliers using IQR
def detect_outliers_iqr(data, column_name):
    Q1 = data[column_name].quantile(0.25)
    Q3 = data[column_name].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = data[(data[column_name] < lower_bound) | (data[column_name] > upper_bound)]

    print(f"\nOutliers in '{column_name}':")
    print(outliers[[column_name]])

    print(f"\nCount of outliers in '{column_name}': {outliers.shape[0]}")
    return outliers

# Detect outliers
bathroom_outliers = detect_outliers_iqr(df, 'Bathrooms')

"""# **Fill missing values in Bathrooms with mean**"""

df['Bathrooms'] = df['Bathrooms'].fillna(df['Bathrooms'].mean())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# --- Step 1: Summary Stats ---
print("Initial Parking Spaces Summary:\n")
print(df['Parking Spaces'].describe())

# --- Step 2: IQR Outlier Detection ---
Q1 = df['Parking Spaces'].quantile(0.25)
Q3 = df['Parking Spaces'].quantile(0.75)
IQR = Q3 - Q1
upper_limit = Q3 + 1.5 * IQR

# View outliers
outliers = df[df['Parking Spaces'] > upper_limit]
print("\nOutliers:\n", outliers[['Parking Spaces', 'Area', 'Location']])

# --- Step 3: Clean Outliers ---
# Option 1: Replace extreme values with NaN
df.loc[df['Parking Spaces'] > upper_limit, 'Parking Spaces'] = np.nan

# --- Step 4: Impute Missing Values ---
# Option: Fill with median
median_parking = df['Parking Spaces'].median()
df['Parking Spaces'].fillna(median_parking, inplace=True)

# --- Step 5: Visualize Before & After ---

# Boxplot (after cleaning)
plt.figure(figsize=(10, 4))
sns.boxplot(x=df['Parking Spaces'])
plt.title("Parking Spaces After Outlier Treatment")
plt.show()

# Histogram (after cleaning & imputation)
plt.figure(figsize=(10, 4))
sns.histplot(df['Parking Spaces'], bins=20, kde=True)
plt.title("Parking Spaces Distribution After Cleaning & Imputation")
plt.show()

# --- Step 6: Categorize Parking Spaces ---
df['Parking Category'] = pd.cut(df['Parking Spaces'],
                                bins=[0, 2, 5, 10, 20, 1000],
                                labels=['Low', 'Moderate', 'High', 'Very High', 'Suspicious'])

print("\nParking Category counts:\n")
print(df['Parking Category'].value_counts())

"""# **Analyzing and Imputing Missing Values in Parking Spaces Column**"""

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew
import pandas as pd

# Convert Parking Spaces to numeric (if it's object)
df['Parking Spaces'] = pd.to_numeric(df['Parking Spaces'], errors='coerce')

# Plot Histogram & Boxplot to check distribution and outliers
plt.figure(figsize=(10, 5))
sns.histplot(df["Parking Spaces"].dropna(), bins=15, kde=True, color='lightgreen')
plt.title("Distribution of Parking Spaces")
plt.xlabel("Number of Parking Spaces")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

plt.figure(figsize=(8, 2))
sns.boxplot(x=df["Parking Spaces"].dropna(), color='tomato')
plt.title("Boxplot of Parking Spaces")
plt.show()

# Check Skewness to decide imputation method
parking_skew = skew(df["Parking Spaces"].dropna())
print(f"Skewness of Parking Spaces: {parking_skew:.2f}")

if parking_skew > 1:
    print("Highly Right-Skewed ‚Üí Use Median for imputation")
elif parking_skew < -1:
    print("Highly Left-Skewed ‚Üí Use Median for imputation")
else:
    print("Approximately Normal ‚Üí Use Mean or Median")

# Impute missing values with median (recommended due to skewness)
df['Parking Spaces'] = df['Parking Spaces'].fillna(df['Parking Spaces'].median())

# Optional: Detect outliers using IQR
def detect_outliers_iqr(data, column_name):
    Q1 = data[column_name].quantile(0.25)
    Q3 = data[column_name].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = data[(data[column_name] < lower_bound) | (data[column_name] > upper_bound)]

    print(f"\nOutliers in '{column_name}':")
    print(outliers[[column_name]])

    print(f"\nCount of outliers in '{column_name}': {outliers.shape[0]}")
    return outliers

# Detect outliers in Parking Spaces
parking_outliers = detect_outliers_iqr(df, 'Parking Spaces')

"""# **Impute missing values with the median:**"""

median_value = df['Parking Spaces'].median()
df['Parking Spaces'] = df['Parking Spaces'].fillna(median_value)

# Show top unique values of Parking Spaces
print("Top 45 unique values in Parking Spaces:")
print(df['Parking Spaces'].dropna().sort_values(ascending=False).unique()[:42])

df['Parking Spaces'].describe()

# See how many rows have Parking Spaces > 1
df[df['Parking Spaces'] > 10][['City', 'Type', 'Price', 'Parking Spaces','Purpose','Title']]

"""# **Cap Parking Spaces to 10**"""

# Cap extreme values in Parking Spaces to 20
df.loc[df['Parking Spaces'] > 10, 'Parking Spaces'] = 10

"""# **Re-check Outliers**"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 2))
sns.boxplot(x=df["Parking Spaces"], color='skyblue')
plt.title("Boxplot of Parking Spaces After Capping")
plt.show()

"""# **Missing Value % servant quarter**

"""

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew

# Histogram
plt.figure(figsize=(10, 5))
sns.histplot(df['Servant Quarters'].dropna(), bins=15, kde=True, color='lightblue')
plt.title("Distribution of Servant Quarters")
plt.grid(True)
plt.show()

# Boxplot
plt.figure(figsize=(8, 2))
sns.boxplot(x=df['Servant Quarters'].dropna(), color='orange')
plt.title("Boxplot of Servant Quarters")
plt.show()

# Skewness
sq_skew = skew(df['Servant Quarters'].dropna())
print(f"Skewness of Servant Quarters: {sq_skew:.2f}")
def detect_outliers_iqr(data, column_name):
    Q1 = data[column_name].quantile(0.25)
    Q3 = data[column_name].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers = data[(data[column_name] < lower) | (data[column_name] > upper)]
    print(f"\nOutliers in {column_name}: {outliers.shape[0]} rows")
    return outliers[[column_name]]

outliers_sq = detect_outliers_iqr(df, 'Servant Quarters')

"""# **Code to Impute**"""

# Fill missing Servant Quarters values with median
df['Servant Quarters'] = df['Servant Quarters'].fillna(df['Servant Quarters'].median())

# Detect outliers again after imputation
outliers_sq_post = detect_outliers_iqr(df, 'Servant Quarters')

# Step 1: Detect outliers using IQR bounds
Q1 = df['Servant Quarters'].quantile(0.25)
Q3 = df['Servant Quarters'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Step 2: Extract full rows from original DataFrame
servant_outliers = df[(df['Servant Quarters'] < lower_bound) | (df['Servant Quarters'] > upper_bound)]

# Step 3: Show detailed info
servant_outliers[['City', 'Type', 'Price', 'Servant Quarters']]

"""# **Show top extreme values only**"""

# Sort by most extreme values
servant_outliers[['City', 'Type', 'Price', 'Servant Quarters','Area','Description']].sort_values(by='Servant Quarters', ascending=False).head(50)

"""# **Capping**"""

# Cap values above 3 servant quarters
df.loc[df['Servant Quarters'] > 3, 'Servant Quarters'] = 3

"""# **Recheck Outliers**"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 2))
sns.boxplot(x=df["Servant Quarters"], color='skyblue')
plt.title("Boxplot of Servant Quarters After Capping")
plt.show()

"""# **Analysis for Kitchens**"""

df['Kitchens'] = pd.to_numeric(df['Kitchens'], errors='coerce')
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew

# Histogram
plt.figure(figsize=(10, 5))
sns.histplot(df["Kitchens"].dropna(), bins=15, kde=True, color='lightgreen')
plt.title("Distribution of Kitchens")
plt.xlabel("Number of Kitchens")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

# Boxplot
plt.figure(figsize=(8, 2))
sns.boxplot(x=df["Kitchens"].dropna(), color='orange')
plt.title("Boxplot of Kitchens")
plt.show()

# Skewness
kitchen_skew = skew(df["Kitchens"].dropna())
print(f"Skewness of Kitchens: {kitchen_skew:.2f}")

if kitchen_skew > 1:
    print("Highly Right-Skewed ‚Üí Use Median for imputation")
elif kitchen_skew < -1:
    print("Highly Left-Skewed ‚Üí Use Median for imputation")
else:
    print("Approximately Normal ‚Üí Use Mean or Median")

    #Detect outliers
def detect_outliers_iqr(data, column_name):
    Q1 = data[column_name].quantile(0.25)
    Q3 = data[column_name].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = data[(data[column_name] < lower_bound) | (data[column_name] > upper_bound)]

    print(f"\nOutliers in '{column_name}':")
    print(outliers[[column_name]])
    print(f"\nCount of outliers in '{column_name}': {outliers.shape[0]}")
    return outliers

# Detect outliers in Kitchens
kitchen_outliers = detect_outliers_iqr(df, 'Kitchens')

"""# **Handle outliers and missing values**"""

# Cap values greater than 3 kitchens
df.loc[df['Kitchens'] > 3, 'Kitchens'] = 3

# Impute missing with median
df['Kitchens'] = df['Kitchens'].fillna(df['Kitchens'].median())

"""# **Recheck Outliers**"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 2))
sns.boxplot(x=df["Servant Quarters"], color='skyblue')
plt.title("Boxplot of Servant Quarters After Capping")
plt.show()

"""# **Cleaning: Store Rooms**"""

df['Store Rooms'] = pd.to_numeric(df['Store Rooms'], errors='coerce')
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew
import pandas as pd

# Step 1: Convert 'Store Rooms' to numeric (in case it's object or string)
df['Store Rooms'] = pd.to_numeric(df['Store Rooms'], errors='coerce')

# Step 2: Plot histogram to check distribution
plt.figure(figsize=(10, 5))
sns.histplot(df["Store Rooms"].dropna(), bins=15, kde=True, color='skyblue')
plt.title("Distribution of Store Rooms")
plt.xlabel("Number of Store Rooms")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

# Step 3: Plot boxplot to visually check outliers
plt.figure(figsize=(8, 2))
sns.boxplot(x=df["Store Rooms"].dropna(), color='orange')
plt.title("Boxplot of Store Rooms")
plt.show()

# Step 4: Check skewness to decide imputation method
store_skew = skew(df["Store Rooms"].dropna())
print(f"Skewness of Store Rooms: {store_skew:.2f}")

# Step 5: Suggest imputation strategy based on skewness
if store_skew > 1:
    print("Highly Right-Skewed ‚Üí Use Median for imputation")
elif store_skew < -1:
    print("Highly Left-Skewed ‚Üí Use Median for imputation")
else:
    print("Approximately Normal ‚Üí Use Mean or Median")

# Step 1: Calculate IQR boundaries for Store Rooms
Q1 = df['Store Rooms'].quantile(0.25)
Q3 = df['Store Rooms'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Step 2: Filter extreme outliers (much higher than upper_bound)
extreme_outliers = df[df['Store Rooms'] > upper_bound]

# Step 3: Show important details of those extreme outliers
extreme_outliers = extreme_outliers[['City', 'Type', 'Price', 'Store Rooms']]
print("üîç Extreme Outliers in Store Rooms:\n")
print(extreme_outliers.sort_values(by='Store Rooms', ascending=False).head(20))  # Top 20 largest

# Step 1: Impute missing values in 'Store Rooms' with the median
median_store_rooms = df['Store Rooms'].median()
df['Store Rooms'] = df['Store Rooms'].fillna(median_store_rooms)

# Step 2: Cap the values at 3 to handle extreme outliers
df['Store Rooms'] = df['Store Rooms'].apply(lambda x: 3 if x > 3 else x)

# Check if any value is still > 3
print("Values greater than 3 in 'Store Rooms':", (df['Store Rooms'] > 3).sum())

"""# **Drop Built in year**"""

df.drop(columns=['Built in year'], inplace=True)

"""# **Handling Missing Values in Type (Property Type)**

"""

# Impute missing values in 'Type' column using the most frequent category (mode)
df['Type'] = df['Type'].fillna(df['Type'].mode()[0])

"""**# Handling Missing Values: 'Price', 'Location', 'Description'**"""

# Fill missing values in 'Area' with 'Unknown'
df['Area'] = df['Area'].fillna("Unknown")

# Fill missing values in 'Description' with 'Unknown'
df['Description'] = df['Description'].fillna("Unknown")

# Fill missing values in 'Type' with 'Unknown'
df['Type'] = df['Type'].fillna("Unknown")

# Fill missing values in 'Purpose' with 'Unknown'
df['Purpose'] = df['Purpose'].fillna("Unknown")

# Fill missing values in 'Location' with 'Unknown'
df['Location'] = df['Location'].fillna("Unknown")

"""# **Drop Title and URL:**"""

# Drop Title and URL columns as they don't add analytical value
df.drop(['Title', 'URL'], axis=1, inplace=True)

"""# **Drop Purpose**

"""

# Normalize the column values to lowercase and strip spaces
df['Purpose'] = df['Purpose'].str.strip().str.lower()

# Apply replacement
df['Purpose'] = df['Purpose'].replace({
    'for sale': 'sale',
    'for': 'rent',
    'unknown': 'unknown'
})

# See the unique cleaned values
print(df['Purpose'].unique())

df['Purpose'] = df['Purpose'].fillna('unknown')
print(df['Purpose'].unique())

print(df['Purpose'].value_counts())

# Step 1: Find the mode of the 'Purpose' column
purpose_mode = df['Purpose'].mode()[0]

# Step 2: Replace 'unknown' and NaN with mode
df['Purpose'] = df['Purpose'].replace('unknown', purpose_mode)
df['Purpose'] = df['Purpose'].fillna(purpose_mode)

# Step 3: Confirm the change
print(df['Purpose'].value_counts())

"""# **View Remaining Columns**"""

# Display remaining column names after cleaning
print("Remaining columns after cleaning:")
print(df.columns.tolist())
# View first 5 rows of the cleaned DataFrame
df.head()

"""# cleans and standardizes the city names in the DataFrame column

# **Install FuzzyWuzzy**
"""

!pip install fuzzywuzzy[speedup]

"""# **Unique cities**"""

unique_cities = df['City'].unique()
print(unique_cities)

from fuzzywuzzy import process

# List of official, clean city names to match against
clean_cities = [
    'Karachi', 'Islamabad', 'Faisalabad', 'Multan', 'Rawalpindi', 'Peshawar',
    'Jhelum', 'Murree', 'Hyderabad', 'Bahawalpur', 'Sialkot', 'Abbottabad',
    'Sahiwal', 'Lahore', 'Gujrat', 'Wah', 'Sargodha', 'Sheikhupura', 'Chakwal',
    'Naran', 'Nowshera', 'Mardan', 'Chilas', 'Sarai Alamgir', 'Quetta',
    'Rahim Yar Khan', 'Okara', 'Attock', 'Dera Ghazi Khan', 'Taxila', 'Sukkur',
    'Gwadar', 'Kasur', 'Mirpur', 'Haripur', 'Gujar Khan', 'Gharo', 'Mirpur Khas',
    'Swat', 'Daska', 'Sadiqabad', 'Buner', 'Jhang', 'Lalamusa', 'Lodhran',
    'Pakpattan', 'Chiniot', 'Dera Ismail Khan', 'Fateh Jang', 'Burewala',
    'Hafizabad', 'Talagang', 'Vehari', 'Jamshoro', 'Galyat', 'Gujranwala',
    'Kharian'
]

def fuzzy_city_correct(city, choices, threshold=85):
    """
    Function to clean a city name using fuzzy matching.

    Args:
        city (str): The city name to clean.
        choices (list): List of clean city names to match against.
        threshold (int): Minimum matching score required to accept a match.

    Returns:
        str: The cleaned city name if matched, otherwise original city.
    """
    if not isinstance(city, str):
        # If the input is not a string (e.g. NaN), return as is
        return city

    # Remove leading and trailing spaces from the city name
    city = city.strip()

    # Use fuzzy matching to find the closest match from the clean city list
    match, score = process.extractOne(city, choices)

    # If the matching score is above the threshold, return the matched city
    if score >= threshold:
        return match
    else:
        # Otherwise, return the original city name unchanged
        return city

# Apply the cleaning function directly to the 'city' column
# This overwrites the original 'city' values with the cleaned versions
df['City'] = df['City'].apply(lambda x: fuzzy_city_correct(x, clean_cities))

# Display the unique cleaned city names after processing
print(df['City'].unique())

"""# **Check unique property types in your data**"""

print(df['Type'].unique())

"""# **Feature Engineering: Additional Calculated Columns**"""

# Convert 'Area' and 'Price' to numeric (will turn non-numeric values into NaN)
df['Area'] = pd.to_numeric(df['Area'], errors='coerce')
df['Price'] = pd.to_numeric(df['Price'], errors='coerce')

"""# **Size (Marla)**
Since 1 Marla ‚âà 225 sq ft:
"""

df['Size (Marla)'] = df['Area'] / 225

"""# **Price per Square Foot**"""

df['Price per sq ft'] = df['Price'] / df['Area']

"""# **Price per Marla**"""

df['Price per Marla'] = df['Price'] / df['Size (Marla)']

df['Size (Marla)'] = df['Size (Marla)'].round(2)
df['Price per sq ft'] = df['Price per sq ft'].replace([np.inf, -np.inf], np.nan).round(0)
df['Price per Marla'] = df['Price per Marla'].replace([np.inf, -np.inf], np.nan).round(0)

df

"""# **Total Rooms**"""

df['Total Rooms'] = df[['Bedrooms','Bathrooms','Kitchens','Store Rooms','Servant Quarters']].sum(axis=1)

"""# **Remove Duplicates from Entire Dataset**"""

# Remove completely identical rows (based on all columns)
df.drop_duplicates(inplace=True)

"""# **BINING**

**Make Bins for Price**
"""

df.loc[:, 'Price'] = pd.to_numeric(df['Price'], errors='coerce')
df.loc[:, 'Bedrooms'] = pd.to_numeric(df['Bedrooms'], errors='coerce')

"""# **Check min/max before binning**"""

print(f"Min price: {df['Price'].min():,.0f}")
print(f"Max price: {df['Price'].max():,.0f}")

"""# **Create Price Bins:**"""

# Define bin ranges and labels
price_bins = [0, 1_000_000, 5_000_000, 10_000_000, 25_000_000, 100_000_000, 1_800_000_000]
price_labels = ['Low', 'Lower-Mid', 'Mid', 'Upper-Mid', 'High-End', 'Ultra-Luxury']

# Create a new column 'Price Category'
df.loc[:, 'Price Category'] = pd.cut(df['Price'], bins=price_bins, labels=price_labels)

# Check distribution
df['Price Category'].value_counts()

"""# **Bedrooms vs Price Category**"""

bedroom_price_bin = df.groupby(['Price Category', 'Bedrooms']).size().unstack().fillna(0).astype(int)
bedroom_price_bin

bedroom_price_bin.plot(kind='bar', stacked=True, figsize=(12,6), colormap='Pastel1')
plt.title('Bedroom Count by Price Category')
plt.xlabel('Bedrooms')
plt.ylabel('Number of Listings')
plt.legend(title='Price Category')
plt.show()

"""# **City & Price Category**"""

city_price_category = df.groupby(['City', 'Price Category']).size().unstack().fillna(0).astype(int)
city_price_category

"""# **Focus on top 10 cities**

"""

top_cities = df['City'].value_counts().head(10).index
city_price_top = city_price_category.loc[top_cities]
city_price_top

luxury_df = df[df['Price Category'].isin(['High-End', 'Ultra-Luxury'])]

luxury_by_city = luxury_df['City'].value_counts().sort_values(ascending=False)

print("üîù Cities with most luxury listings:")
print(luxury_by_city.head(20))

luxury_by_city.head(10).plot(kind='bar', figsize=(10, 5), color='gold')
plt.title('Top Cities with High-End and Ultra-Luxury Listings')
plt.xlabel('City')
plt.ylabel('Number of Luxury Listings')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Which Property Types Are Most Common in Each City?**"""

city_type_counts = df.groupby(['City', 'Type']).size().unstack().fillna(0).astype(int)
city_type_counts

"""**Filter Dataset for Flats Only**"""

flats_df = df[df['Type'] == 'Flat']
flats_by_city = flats_df['City'].value_counts().sort_values(ascending=False)
print("üîù Cities with the Most Flat Listings:")
print(flats_by_city.head(10))

top_property_types = df['Type'].value_counts().head(10).index.tolist()
print(top_property_types)

for prop_type in top_property_types:
    print(f"\nüîç {prop_type.upper()} Analysis")

    # Filter for the property type
    type_df = df[df['Type'] == prop_type]

    # 1. Top Cities by Listing Count
    city_counts = type_df['City'].value_counts().head(5)
    print("üìä Top Cities by Number of Listings:")
    print(city_counts)

    # 2. Avg Price of that Type by City
    avg_price_by_city = type_df.groupby('City')['Price'].mean().sort_values(ascending=False).round(0).head(5)
    print("\nüí∞ Top Cities by Average Price:")
    print(avg_price_by_city)

    # 3. Luxury Listings of this type
    luxury_df = type_df[type_df['Price Category'].isin(['High-End', 'Luxury', 'Ultra Luxury'])]
    luxury_counts = luxury_df['City'].value_counts().head(5)
    print("\nüèôÔ∏è Cities with Most Luxury Listings of this Type:")
    print(luxury_counts)

"""# **Price Trends by Bedrooms**"""

bedroom_price = df.groupby('Bedrooms')['Price'].mean().sort_index()
bedroom_price.plot(kind='line', marker='o', title='Average Price vs Bedrooms')

import seaborn as sns
import matplotlib.pyplot as plt
avg_ppsqft = df.groupby(['City', 'Type'])['Price_per_sqft'].mean().unstack().fillna(0)
top_cities = df['City'].value_counts().head(10).index
avg_ppsqft_top = avg_ppsqft.loc[top_cities]
plt.figure(figsize=(12, 6))
sns.heatmap(avg_ppsqft_top, annot=True, fmt=".0f", cmap="YlGnBu")
plt.title("Average Price per Sqft by City and Property Type")
plt.xlabel("Property Type")
plt.ylabel("City")
plt.tight_layout()
plt.show()

# Step 1: Save the cleaned DataFrame to a CSV file
df.to_csv('zameen_cleaned_data.csv', index=False)

# Step 2: Provide a download link (for Colab)
from google.colab import files
files.download('zameen_cleaned_data.csv')